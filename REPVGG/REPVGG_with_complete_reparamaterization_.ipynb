{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REPVGG_with_complete_reparamaterization .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJBqQgu3jRV4"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import os\n",
        "import time\n",
        "import importlib\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "import logging\n",
        "import argparse\n",
        "import numpy as np \n",
        "import random\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.backends.cudnn\n",
        "import torchvision.utils\n",
        "import torch.nn.functional as F\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time \n",
        "import torchvision.transforms as transforms\n",
        "import torchvision"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koy4hCaKdRbT"
      },
      "source": [
        "MAIN REPVGG Apart from main architecture specification I have also included  Reparameterization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "OCrpxcHrnahF"
      },
      "source": [
        "def fcbn(conv: nn.Conv2d, bn: nn.BatchNorm2d):\n",
        "\n",
        "          sf = bn.weight.data / torch.sqrt(bn.running_var + bn.eps) \n",
        "          fb = bn.bias.data - sf * bn.running_mean\n",
        "          fb = fb + sf * conv.bias.data\n",
        "          fk = sf.view(-1, 1, 1, 1) * conv.weight.data\n",
        "\n",
        "          return fk, fb\n",
        "\n",
        "class block(nn.Module):\n",
        "\n",
        "    def __init__(self,in_channels=None,out_channels=None,stride=None):\n",
        "        super(block,self).__init__()\n",
        "        self.stride = stride \n",
        "        self.in_channels = in_channels \n",
        "        self.out_channels = out_channels\n",
        "        self.nl = nn.ReLU()\n",
        "        #self.s = nn.Identity()\n",
        "        b1 = [nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels,\n",
        "                                                  kernel_size=(3,3), stride=self.stride, padding=(1,1)),nn.BatchNorm2d(num_features=self.out_channels)]\n",
        "        self.x = nn.Sequential(*b1)\n",
        "        b2 = [nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels,\n",
        "                                                  kernel_size=(1,1), stride=self.stride, padding=(0,0)),nn.BatchNorm2d(num_features=self.out_channels)]\n",
        "        self.y = nn.Sequential(*b2)\n",
        "        self.branches = nn.ModuleList([nn.Sequential(*b1),nn.Sequential(*b2)])\n",
        "        if self.stride == (1,1):\n",
        "          self.z = nn.BatchNorm2d(num_features=self.in_channels)\n",
        "          self.branches.append(nn.BatchNorm2d(num_features=self.in_channels))\n",
        "        else :\n",
        "          self.z =None\n",
        "\n",
        "\n",
        "          \n",
        "\n",
        "    def reparam(self):\n",
        "        in_ch = self.branches[0][0].weight.data.shape[1]\n",
        "        out_ch = self.branches[0][0].weight.data.shape[0]\n",
        "\n",
        "        r = nn.Conv2d(in_channels = in_ch , out_channels =  out_ch,kernel_size= (3,3),\n",
        "                        padding=(1,1), bias=True, stride=self.branches[0][0].stride)\n",
        "\n",
        "        fk3 , fb3 = fcbn(*self.branches[0])\n",
        "        fk1, fb1 = fcbn(*self.branches[1])\n",
        "        rwd = fk3\n",
        "        rbd = fb3\n",
        "        rwd[..., 1:2, 1:2] = rwd[..., 1:2, 1:2] + fk1\n",
        "\n",
        "        #in case we apply bn layer\n",
        "        if len(self.branches) == 3:\n",
        "            sf = self.branches[2].weight.data / (self.branches[2].running_var + self.branches[2].eps).sqrt()\n",
        "            rwd[range(out_ch), range(in_ch), 1, 1] += sf\n",
        "            rbd = rbd + self.branches[2].bias.data\n",
        "            rbd = rbd - sf * self.branches[2].running_mean \n",
        "\n",
        "        self.branches = nn.ModuleList([r])\n",
        "\n",
        "\n",
        "    def forward(self,input):\n",
        "      #case of reparametrization \n",
        "        if len(self.branches)==1 :\n",
        "          out = self.branches[0](input)\n",
        "        else :\n",
        "           if self.z == None :\n",
        "             out = self.x(input) + self.y(input)\n",
        "           else :\n",
        "             out = self.x(input) + self.y(input) +self.z(input)\n",
        "             out = self.nl(out)\n",
        "        return out \n",
        "\n",
        "\n",
        "class REPVGG(nn.Module):\n",
        "\n",
        "          def __init__(self,in_channels=3,num_classes=10,blocks=None,multipl=None):\n",
        "                    super(REPVGG,self).__init__()\n",
        "                    self.blocks= blocks\n",
        "                    self.multipl= multipl \n",
        "                    self.in_channels = in_channels\n",
        "                    #self.main_REPVGG = self.main_architecture()\n",
        "                    self.g = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "                    self.linear = nn.Linear(int(512 * multipl[3]), num_classes)                               \n",
        "                    layers = []\n",
        "\n",
        "\n",
        "                    #stage0\n",
        "\n",
        "                    \n",
        "                    multipl = self.multipl\n",
        "                    out_channels = min(64,int(64*multipl[0])) \n",
        "                    in_channels = self.in_channels\n",
        "                    blocks = self.blocks\n",
        "                    layers += [block(in_channels = in_channels,out_channels=out_channels,stride = (2,2))]\n",
        "                    in_channels = min(64, int(64*multipl[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                #stage 1,2 ,3 4 \n",
        "\n",
        "\n",
        "\n",
        "                    for i in range(4):\n",
        "\n",
        "                      out_channels = int(64*(2**i) * multipl[i])\n",
        "                      layers += [block(in_channels = in_channels,out_channels=out_channels,stride = (2,2))]\n",
        "                      in_channels = out_channels\n",
        "\n",
        "                      for j in range(blocks[i]-1):\n",
        "                        layers += [block(in_channels = in_channels,out_channels=out_channels,stride = (1,1))]\n",
        "                      in_channels = out_channels \n",
        "                    self.layers = layers\n",
        "                    self.main_REPVGG = nn.Sequential(*layers)\n",
        "          \n",
        "          def reparametrize(self):\n",
        "                    for  block in self.layers:\n",
        "                            block.reparam()\n",
        "\n",
        "\n",
        "          def forward(self,x):\n",
        "            x = self.main_REPVGG(x)\n",
        "            x = self.g(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self.linear(x)\n",
        "            return x"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-QNUDXPthKf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To2a2qRGfCu6",
        "outputId": "2649220d-e47b-49ce-c469-2701bbe1c74a"
      },
      "source": [
        "transform = transforms.Compose([transforms.Pad(4),transforms.RandomHorizontalFlip(),transforms.RandomCrop(32),transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "test_transform  = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../../data/', train=True,transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../../data/', train=False,transform=test_transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=100, shuffle=False)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpuncoWyZ5px"
      },
      "source": [
        "### **DIFFERENT TYPES OF REPVGG I have only implemented A1 BECAUSE IT IS SMALLEST but it is recommended to use B2 architecture since according to paper it giver best top1 accuracy all other can be implemented just by changing the arguments  **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N-oULfhZqcK"
      },
      "source": [
        "![download (3).png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OvYBVoMmeuE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f202a3-eaeb-4d92-a4cf-963fa2905b7d"
      },
      "source": [
        "depth = 2\n",
        "epochs = 80\n",
        "batch_size = 256\n",
        "base_lr = 0.01\n",
        "lr_decay = 0.0001\n",
        "milestones = '[80, 120]'\n",
        "device = \"cuda\"\n",
        "num_workers = 3\n",
        "\n",
        "model = REPVGG(in_channels=3,num_classes=10,blocks=[2, 4, 14, 1],multipl=[0.75, 0.75, 0.75, 2.5]).to(device)\n",
        "#model = VGG(in_channels = 3, num_classes = 10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=base_lr)\n",
        "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=lr_decay)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=lr_decay)\n",
        "total_time = 0 \n",
        "for epoch in range(epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    start_time = time.time()\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (i+1) % 250 == 0:\n",
        "      elapsed_time = time.time() - start_time\n",
        "      total_time += elapsed_time\n",
        "      print (\"Epoch {}, Step {} Loss: {:.4f} time : {:.4f}min\".format(epoch+1, i+1, loss.item(),total_time))\n",
        " # scheduler.step()\n",
        "\n",
        " "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Step 250 Loss: 2.8956 time : 0.1427min\n",
            "Epoch 1, Step 500 Loss: 1.8745 time : 0.2820min\n",
            "Epoch 2, Step 250 Loss: 1.9382 time : 0.4220min\n",
            "Epoch 2, Step 500 Loss: 1.7348 time : 0.5610min\n",
            "Epoch 3, Step 250 Loss: 1.5214 time : 0.7063min\n",
            "Epoch 3, Step 500 Loss: 1.5143 time : 0.8460min\n",
            "Epoch 4, Step 250 Loss: 1.6054 time : 0.9865min\n",
            "Epoch 4, Step 500 Loss: 1.6148 time : 1.1241min\n",
            "Epoch 5, Step 250 Loss: 1.5200 time : 1.2645min\n",
            "Epoch 5, Step 500 Loss: 1.5281 time : 1.4041min\n",
            "Epoch 6, Step 250 Loss: 1.1334 time : 1.5422min\n",
            "Epoch 6, Step 500 Loss: 1.3406 time : 1.6818min\n",
            "Epoch 7, Step 250 Loss: 1.0562 time : 1.8209min\n",
            "Epoch 7, Step 500 Loss: 1.3822 time : 1.9615min\n",
            "Epoch 8, Step 250 Loss: 1.0852 time : 2.1022min\n",
            "Epoch 8, Step 500 Loss: 1.1502 time : 2.2419min\n",
            "Epoch 9, Step 250 Loss: 1.1184 time : 2.3815min\n",
            "Epoch 9, Step 500 Loss: 0.9258 time : 2.5206min\n",
            "Epoch 10, Step 250 Loss: 1.0088 time : 2.6610min\n",
            "Epoch 10, Step 500 Loss: 1.0270 time : 2.7994min\n",
            "Epoch 11, Step 250 Loss: 1.1338 time : 2.9394min\n",
            "Epoch 11, Step 500 Loss: 1.1622 time : 3.0839min\n",
            "Epoch 12, Step 250 Loss: 1.0082 time : 3.2321min\n",
            "Epoch 12, Step 500 Loss: 0.9516 time : 3.3712min\n",
            "Epoch 13, Step 250 Loss: 0.9509 time : 3.5104min\n",
            "Epoch 13, Step 500 Loss: 0.8193 time : 3.6488min\n",
            "Epoch 14, Step 250 Loss: 1.0601 time : 3.7892min\n",
            "Epoch 14, Step 500 Loss: 0.9877 time : 3.9259min\n",
            "Epoch 15, Step 250 Loss: 0.9156 time : 4.0662min\n",
            "Epoch 15, Step 500 Loss: 0.7931 time : 4.2063min\n",
            "Epoch 16, Step 250 Loss: 0.6398 time : 4.3432min\n",
            "Epoch 16, Step 500 Loss: 0.8477 time : 4.4805min\n",
            "Epoch 17, Step 250 Loss: 0.8984 time : 4.6222min\n",
            "Epoch 17, Step 500 Loss: 1.0243 time : 4.7619min\n",
            "Epoch 18, Step 250 Loss: 0.8798 time : 4.9024min\n",
            "Epoch 18, Step 500 Loss: 1.0918 time : 5.0415min\n",
            "Epoch 19, Step 250 Loss: 0.9398 time : 5.1820min\n",
            "Epoch 19, Step 500 Loss: 0.8199 time : 5.3220min\n",
            "Epoch 20, Step 250 Loss: 0.8688 time : 5.4627min\n",
            "Epoch 20, Step 500 Loss: 0.8425 time : 5.6032min\n",
            "Epoch 21, Step 250 Loss: 0.7252 time : 5.7418min\n",
            "Epoch 21, Step 500 Loss: 0.7516 time : 5.8821min\n",
            "Epoch 22, Step 250 Loss: 0.6750 time : 6.0221min\n",
            "Epoch 22, Step 500 Loss: 0.8650 time : 6.1669min\n",
            "Epoch 23, Step 250 Loss: 0.6527 time : 6.3056min\n",
            "Epoch 23, Step 500 Loss: 0.8883 time : 6.4465min\n",
            "Epoch 24, Step 250 Loss: 0.9068 time : 6.5865min\n",
            "Epoch 24, Step 500 Loss: 0.6222 time : 6.7245min\n",
            "Epoch 25, Step 250 Loss: 0.6534 time : 6.8650min\n",
            "Epoch 25, Step 500 Loss: 0.6930 time : 7.0048min\n",
            "Epoch 26, Step 250 Loss: 0.5516 time : 7.1462min\n",
            "Epoch 26, Step 500 Loss: 0.5492 time : 7.2863min\n",
            "Epoch 27, Step 250 Loss: 0.7462 time : 7.4291min\n",
            "Epoch 27, Step 500 Loss: 0.7475 time : 7.5685min\n",
            "Epoch 28, Step 250 Loss: 0.6979 time : 7.7105min\n",
            "Epoch 28, Step 500 Loss: 0.7751 time : 7.8500min\n",
            "Epoch 29, Step 250 Loss: 0.8349 time : 7.9894min\n",
            "Epoch 29, Step 500 Loss: 0.6585 time : 8.1261min\n",
            "Epoch 30, Step 250 Loss: 0.6584 time : 8.2688min\n",
            "Epoch 30, Step 500 Loss: 0.6517 time : 8.4078min\n",
            "Epoch 31, Step 250 Loss: 0.4077 time : 8.5491min\n",
            "Epoch 31, Step 500 Loss: 0.6503 time : 8.6870min\n",
            "Epoch 32, Step 250 Loss: 0.7363 time : 8.8297min\n",
            "Epoch 32, Step 500 Loss: 1.5835 time : 8.9687min\n",
            "Epoch 33, Step 250 Loss: 0.8364 time : 9.1083min\n",
            "Epoch 33, Step 500 Loss: 0.7940 time : 9.2493min\n",
            "Epoch 34, Step 250 Loss: 0.6648 time : 9.3920min\n",
            "Epoch 34, Step 500 Loss: 0.7766 time : 9.5299min\n",
            "Epoch 35, Step 250 Loss: 0.5898 time : 9.6719min\n",
            "Epoch 35, Step 500 Loss: 0.6545 time : 9.8142min\n",
            "Epoch 36, Step 250 Loss: 0.6201 time : 9.9544min\n",
            "Epoch 36, Step 500 Loss: 0.4775 time : 10.0927min\n",
            "Epoch 37, Step 250 Loss: 0.7740 time : 10.2358min\n",
            "Epoch 37, Step 500 Loss: 0.7822 time : 10.3735min\n",
            "Epoch 38, Step 250 Loss: 0.6002 time : 10.5114min\n",
            "Epoch 38, Step 500 Loss: 0.6963 time : 10.6506min\n",
            "Epoch 39, Step 250 Loss: 0.6216 time : 10.7912min\n",
            "Epoch 39, Step 500 Loss: 0.7833 time : 10.9270min\n",
            "Epoch 40, Step 250 Loss: 0.5338 time : 11.0673min\n",
            "Epoch 40, Step 500 Loss: 0.6682 time : 11.2062min\n",
            "Epoch 41, Step 250 Loss: 0.6581 time : 11.3481min\n",
            "Epoch 41, Step 500 Loss: 0.6200 time : 11.4853min\n",
            "Epoch 42, Step 250 Loss: 0.5110 time : 11.6234min\n",
            "Epoch 42, Step 500 Loss: 0.4670 time : 11.7608min\n",
            "Epoch 43, Step 250 Loss: 0.5822 time : 11.8979min\n",
            "Epoch 43, Step 500 Loss: 0.6045 time : 12.0370min\n",
            "Epoch 44, Step 250 Loss: 0.7059 time : 12.1770min\n",
            "Epoch 44, Step 500 Loss: 0.7288 time : 12.3178min\n",
            "Epoch 45, Step 250 Loss: 0.5598 time : 12.4587min\n",
            "Epoch 45, Step 500 Loss: 0.6018 time : 12.6025min\n",
            "Epoch 46, Step 250 Loss: 0.8102 time : 12.7435min\n",
            "Epoch 46, Step 500 Loss: 0.5366 time : 12.8928min\n",
            "Epoch 47, Step 250 Loss: 0.7195 time : 13.0357min\n",
            "Epoch 47, Step 500 Loss: 0.4579 time : 13.1749min\n",
            "Epoch 48, Step 250 Loss: 0.4839 time : 13.3136min\n",
            "Epoch 48, Step 500 Loss: 0.5316 time : 13.4519min\n",
            "Epoch 49, Step 250 Loss: 0.5226 time : 13.5923min\n",
            "Epoch 49, Step 500 Loss: 0.5360 time : 13.7317min\n",
            "Epoch 50, Step 250 Loss: 0.4714 time : 13.8704min\n",
            "Epoch 50, Step 500 Loss: 0.4862 time : 14.0083min\n",
            "Epoch 51, Step 250 Loss: 0.5427 time : 14.1440min\n",
            "Epoch 51, Step 500 Loss: 0.5288 time : 14.2829min\n",
            "Epoch 52, Step 250 Loss: 0.5697 time : 14.4214min\n",
            "Epoch 52, Step 500 Loss: 0.5039 time : 14.5599min\n",
            "Epoch 53, Step 250 Loss: 0.6287 time : 14.6989min\n",
            "Epoch 53, Step 500 Loss: 0.7311 time : 14.8366min\n",
            "Epoch 54, Step 250 Loss: 0.5297 time : 14.9772min\n",
            "Epoch 54, Step 500 Loss: 0.5373 time : 15.1164min\n",
            "Epoch 55, Step 250 Loss: 0.6685 time : 15.2571min\n",
            "Epoch 55, Step 500 Loss: 0.6834 time : 15.3966min\n",
            "Epoch 56, Step 250 Loss: 0.6105 time : 15.5363min\n",
            "Epoch 56, Step 500 Loss: 0.7178 time : 15.6758min\n",
            "Epoch 57, Step 250 Loss: 0.5574 time : 15.8147min\n",
            "Epoch 57, Step 500 Loss: 0.5945 time : 15.9522min\n",
            "Epoch 58, Step 250 Loss: 0.3600 time : 16.0914min\n",
            "Epoch 58, Step 500 Loss: 0.4200 time : 16.2292min\n",
            "Epoch 59, Step 250 Loss: 0.4408 time : 16.3675min\n",
            "Epoch 59, Step 500 Loss: 0.5649 time : 16.5068min\n",
            "Epoch 60, Step 250 Loss: 0.5283 time : 16.6458min\n",
            "Epoch 60, Step 500 Loss: 0.4989 time : 16.7840min\n",
            "Epoch 61, Step 250 Loss: 0.5967 time : 16.9243min\n",
            "Epoch 61, Step 500 Loss: 0.5200 time : 17.0631min\n",
            "Epoch 62, Step 250 Loss: 0.3970 time : 17.2028min\n",
            "Epoch 62, Step 500 Loss: 0.7311 time : 17.3484min\n",
            "Epoch 63, Step 250 Loss: 0.4456 time : 17.4879min\n",
            "Epoch 63, Step 500 Loss: 0.4584 time : 17.6260min\n",
            "Epoch 64, Step 250 Loss: 0.2934 time : 17.7652min\n",
            "Epoch 64, Step 500 Loss: 0.5873 time : 17.9031min\n",
            "Epoch 65, Step 250 Loss: 0.5186 time : 18.0426min\n",
            "Epoch 65, Step 500 Loss: 0.3072 time : 18.1805min\n",
            "Epoch 66, Step 250 Loss: 0.5054 time : 18.3192min\n",
            "Epoch 66, Step 500 Loss: 0.4915 time : 18.4583min\n",
            "Epoch 67, Step 250 Loss: 0.4930 time : 18.5971min\n",
            "Epoch 67, Step 500 Loss: 0.8601 time : 18.7358min\n",
            "Epoch 68, Step 250 Loss: 0.5648 time : 18.8745min\n",
            "Epoch 68, Step 500 Loss: 0.4975 time : 19.0118min\n",
            "Epoch 69, Step 250 Loss: 0.3605 time : 19.1505min\n",
            "Epoch 69, Step 500 Loss: 0.4507 time : 19.2884min\n",
            "Epoch 70, Step 250 Loss: 0.2962 time : 19.4272min\n",
            "Epoch 70, Step 500 Loss: 0.3984 time : 19.5662min\n",
            "Epoch 71, Step 250 Loss: 0.4037 time : 19.7058min\n",
            "Epoch 71, Step 500 Loss: 0.4872 time : 19.8449min\n",
            "Epoch 72, Step 250 Loss: 0.4263 time : 19.9852min\n",
            "Epoch 72, Step 500 Loss: 0.2845 time : 20.1231min\n",
            "Epoch 73, Step 250 Loss: 0.3861 time : 20.2635min\n",
            "Epoch 73, Step 500 Loss: 0.4690 time : 20.4025min\n",
            "Epoch 74, Step 250 Loss: 0.3590 time : 20.5429min\n",
            "Epoch 74, Step 500 Loss: 0.3614 time : 20.6813min\n",
            "Epoch 75, Step 250 Loss: 0.2801 time : 20.8216min\n",
            "Epoch 75, Step 500 Loss: 0.3676 time : 20.9596min\n",
            "Epoch 76, Step 250 Loss: 0.3320 time : 21.0994min\n",
            "Epoch 76, Step 500 Loss: 0.5243 time : 21.2379min\n",
            "Epoch 77, Step 250 Loss: 0.4099 time : 21.3786min\n",
            "Epoch 77, Step 500 Loss: 0.4116 time : 21.5173min\n",
            "Epoch 78, Step 250 Loss: 0.6175 time : 21.6561min\n",
            "Epoch 78, Step 500 Loss: 0.6535 time : 21.7941min\n",
            "Epoch 79, Step 250 Loss: 0.3800 time : 21.9335min\n",
            "Epoch 79, Step 500 Loss: 0.2793 time : 22.0713min\n",
            "Epoch 80, Step 250 Loss: 0.5469 time : 22.2104min\n",
            "Epoch 80, Step 500 Loss: 0.3248 time : 22.3485min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9LsIAWoy7co",
        "outputId": "9c9ca6e8-c599-4bc7-e279-c73db16f7194"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct =0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy ( test images ) : {} %'.format(100 * correct / total))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy ( test images ) : 87.04 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytg3yZVvfOZT",
        "outputId": "463dc8dd-5ee4-42de-ea05-ca61eeb37725"
      },
      "source": [
        "#before model.reparametrized\n",
        "\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "model_stats = summary(model, (3, 224, 224))\n",
        "summary_str = str(model_stats)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 48, 112, 112]           1,344\n",
            "            Conv2d-2         [-1, 48, 112, 112]           1,344\n",
            "       BatchNorm2d-3         [-1, 48, 112, 112]              96\n",
            "       BatchNorm2d-4         [-1, 48, 112, 112]              96\n",
            "            Conv2d-5         [-1, 48, 112, 112]             192\n",
            "            Conv2d-6         [-1, 48, 112, 112]             192\n",
            "       BatchNorm2d-7         [-1, 48, 112, 112]              96\n",
            "       BatchNorm2d-8         [-1, 48, 112, 112]              96\n",
            "             block-9         [-1, 48, 112, 112]               0\n",
            "           Conv2d-10           [-1, 48, 56, 56]          20,784\n",
            "           Conv2d-11           [-1, 48, 56, 56]          20,784\n",
            "      BatchNorm2d-12           [-1, 48, 56, 56]              96\n",
            "      BatchNorm2d-13           [-1, 48, 56, 56]              96\n",
            "           Conv2d-14           [-1, 48, 56, 56]           2,352\n",
            "           Conv2d-15           [-1, 48, 56, 56]           2,352\n",
            "      BatchNorm2d-16           [-1, 48, 56, 56]              96\n",
            "      BatchNorm2d-17           [-1, 48, 56, 56]              96\n",
            "            block-18           [-1, 48, 56, 56]               0\n",
            "           Conv2d-19           [-1, 48, 56, 56]          20,784\n",
            "           Conv2d-20           [-1, 48, 56, 56]          20,784\n",
            "      BatchNorm2d-21           [-1, 48, 56, 56]              96\n",
            "      BatchNorm2d-22           [-1, 48, 56, 56]              96\n",
            "           Conv2d-23           [-1, 48, 56, 56]           2,352\n",
            "           Conv2d-24           [-1, 48, 56, 56]           2,352\n",
            "      BatchNorm2d-25           [-1, 48, 56, 56]              96\n",
            "      BatchNorm2d-26           [-1, 48, 56, 56]              96\n",
            "      BatchNorm2d-27           [-1, 48, 56, 56]              96\n",
            "             ReLU-28           [-1, 48, 56, 56]               0\n",
            "            block-29           [-1, 48, 56, 56]               0\n",
            "           Conv2d-30           [-1, 96, 28, 28]          41,568\n",
            "           Conv2d-31           [-1, 96, 28, 28]          41,568\n",
            "      BatchNorm2d-32           [-1, 96, 28, 28]             192\n",
            "      BatchNorm2d-33           [-1, 96, 28, 28]             192\n",
            "           Conv2d-34           [-1, 96, 28, 28]           4,704\n",
            "           Conv2d-35           [-1, 96, 28, 28]           4,704\n",
            "      BatchNorm2d-36           [-1, 96, 28, 28]             192\n",
            "      BatchNorm2d-37           [-1, 96, 28, 28]             192\n",
            "            block-38           [-1, 96, 28, 28]               0\n",
            "           Conv2d-39           [-1, 96, 28, 28]          83,040\n",
            "           Conv2d-40           [-1, 96, 28, 28]          83,040\n",
            "      BatchNorm2d-41           [-1, 96, 28, 28]             192\n",
            "      BatchNorm2d-42           [-1, 96, 28, 28]             192\n",
            "           Conv2d-43           [-1, 96, 28, 28]           9,312\n",
            "           Conv2d-44           [-1, 96, 28, 28]           9,312\n",
            "      BatchNorm2d-45           [-1, 96, 28, 28]             192\n",
            "      BatchNorm2d-46           [-1, 96, 28, 28]             192\n",
            "      BatchNorm2d-47           [-1, 96, 28, 28]             192\n",
            "             ReLU-48           [-1, 96, 28, 28]               0\n",
            "            block-49           [-1, 96, 28, 28]               0\n",
            "           Conv2d-50           [-1, 96, 28, 28]          83,040\n",
            "           Conv2d-51           [-1, 96, 28, 28]          83,040\n",
            "      BatchNorm2d-52           [-1, 96, 28, 28]             192\n",
            "      BatchNorm2d-53           [-1, 96, 28, 28]             192\n",
            "           Conv2d-54           [-1, 96, 28, 28]           9,312\n",
            "           Conv2d-55           [-1, 96, 28, 28]           9,312\n",
            "      BatchNorm2d-56           [-1, 96, 28, 28]             192\n",
            "      BatchNorm2d-57           [-1, 96, 28, 28]             192\n",
            "      BatchNorm2d-58           [-1, 96, 28, 28]             192\n",
            "             ReLU-59           [-1, 96, 28, 28]               0\n",
            "            block-60           [-1, 96, 28, 28]               0\n",
            "           Conv2d-61           [-1, 96, 28, 28]          83,040\n",
            "           Conv2d-62           [-1, 96, 28, 28]          83,040\n",
            "      BatchNorm2d-63           [-1, 96, 28, 28]             192\n",
            "      BatchNorm2d-64           [-1, 96, 28, 28]             192\n",
            "           Conv2d-65           [-1, 96, 28, 28]           9,312\n",
            "           Conv2d-66           [-1, 96, 28, 28]           9,312\n",
            "      BatchNorm2d-67           [-1, 96, 28, 28]             192\n",
            "      BatchNorm2d-68           [-1, 96, 28, 28]             192\n",
            "      BatchNorm2d-69           [-1, 96, 28, 28]             192\n",
            "             ReLU-70           [-1, 96, 28, 28]               0\n",
            "            block-71           [-1, 96, 28, 28]               0\n",
            "           Conv2d-72          [-1, 192, 14, 14]         166,080\n",
            "           Conv2d-73          [-1, 192, 14, 14]         166,080\n",
            "      BatchNorm2d-74          [-1, 192, 14, 14]             384\n",
            "      BatchNorm2d-75          [-1, 192, 14, 14]             384\n",
            "           Conv2d-76          [-1, 192, 14, 14]          18,624\n",
            "           Conv2d-77          [-1, 192, 14, 14]          18,624\n",
            "      BatchNorm2d-78          [-1, 192, 14, 14]             384\n",
            "      BatchNorm2d-79          [-1, 192, 14, 14]             384\n",
            "            block-80          [-1, 192, 14, 14]               0\n",
            "           Conv2d-81          [-1, 192, 14, 14]         331,968\n",
            "           Conv2d-82          [-1, 192, 14, 14]         331,968\n",
            "      BatchNorm2d-83          [-1, 192, 14, 14]             384\n",
            "      BatchNorm2d-84          [-1, 192, 14, 14]             384\n",
            "           Conv2d-85          [-1, 192, 14, 14]          37,056\n",
            "           Conv2d-86          [-1, 192, 14, 14]          37,056\n",
            "      BatchNorm2d-87          [-1, 192, 14, 14]             384\n",
            "      BatchNorm2d-88          [-1, 192, 14, 14]             384\n",
            "      BatchNorm2d-89          [-1, 192, 14, 14]             384\n",
            "             ReLU-90          [-1, 192, 14, 14]               0\n",
            "            block-91          [-1, 192, 14, 14]               0\n",
            "           Conv2d-92          [-1, 192, 14, 14]         331,968\n",
            "           Conv2d-93          [-1, 192, 14, 14]         331,968\n",
            "      BatchNorm2d-94          [-1, 192, 14, 14]             384\n",
            "      BatchNorm2d-95          [-1, 192, 14, 14]             384\n",
            "           Conv2d-96          [-1, 192, 14, 14]          37,056\n",
            "           Conv2d-97          [-1, 192, 14, 14]          37,056\n",
            "      BatchNorm2d-98          [-1, 192, 14, 14]             384\n",
            "      BatchNorm2d-99          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-100          [-1, 192, 14, 14]             384\n",
            "            ReLU-101          [-1, 192, 14, 14]               0\n",
            "           block-102          [-1, 192, 14, 14]               0\n",
            "          Conv2d-103          [-1, 192, 14, 14]         331,968\n",
            "          Conv2d-104          [-1, 192, 14, 14]         331,968\n",
            "     BatchNorm2d-105          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-106          [-1, 192, 14, 14]             384\n",
            "          Conv2d-107          [-1, 192, 14, 14]          37,056\n",
            "          Conv2d-108          [-1, 192, 14, 14]          37,056\n",
            "     BatchNorm2d-109          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-110          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-111          [-1, 192, 14, 14]             384\n",
            "            ReLU-112          [-1, 192, 14, 14]               0\n",
            "           block-113          [-1, 192, 14, 14]               0\n",
            "          Conv2d-114          [-1, 192, 14, 14]         331,968\n",
            "          Conv2d-115          [-1, 192, 14, 14]         331,968\n",
            "     BatchNorm2d-116          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-117          [-1, 192, 14, 14]             384\n",
            "          Conv2d-118          [-1, 192, 14, 14]          37,056\n",
            "          Conv2d-119          [-1, 192, 14, 14]          37,056\n",
            "     BatchNorm2d-120          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-121          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-122          [-1, 192, 14, 14]             384\n",
            "            ReLU-123          [-1, 192, 14, 14]               0\n",
            "           block-124          [-1, 192, 14, 14]               0\n",
            "          Conv2d-125          [-1, 192, 14, 14]         331,968\n",
            "          Conv2d-126          [-1, 192, 14, 14]         331,968\n",
            "     BatchNorm2d-127          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-128          [-1, 192, 14, 14]             384\n",
            "          Conv2d-129          [-1, 192, 14, 14]          37,056\n",
            "          Conv2d-130          [-1, 192, 14, 14]          37,056\n",
            "     BatchNorm2d-131          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-132          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-133          [-1, 192, 14, 14]             384\n",
            "            ReLU-134          [-1, 192, 14, 14]               0\n",
            "           block-135          [-1, 192, 14, 14]               0\n",
            "          Conv2d-136          [-1, 192, 14, 14]         331,968\n",
            "          Conv2d-137          [-1, 192, 14, 14]         331,968\n",
            "     BatchNorm2d-138          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-139          [-1, 192, 14, 14]             384\n",
            "          Conv2d-140          [-1, 192, 14, 14]          37,056\n",
            "          Conv2d-141          [-1, 192, 14, 14]          37,056\n",
            "     BatchNorm2d-142          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-143          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-144          [-1, 192, 14, 14]             384\n",
            "            ReLU-145          [-1, 192, 14, 14]               0\n",
            "           block-146          [-1, 192, 14, 14]               0\n",
            "          Conv2d-147          [-1, 192, 14, 14]         331,968\n",
            "          Conv2d-148          [-1, 192, 14, 14]         331,968\n",
            "     BatchNorm2d-149          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-150          [-1, 192, 14, 14]             384\n",
            "          Conv2d-151          [-1, 192, 14, 14]          37,056\n",
            "          Conv2d-152          [-1, 192, 14, 14]          37,056\n",
            "     BatchNorm2d-153          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-154          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-155          [-1, 192, 14, 14]             384\n",
            "            ReLU-156          [-1, 192, 14, 14]               0\n",
            "           block-157          [-1, 192, 14, 14]               0\n",
            "          Conv2d-158          [-1, 192, 14, 14]         331,968\n",
            "          Conv2d-159          [-1, 192, 14, 14]         331,968\n",
            "     BatchNorm2d-160          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-161          [-1, 192, 14, 14]             384\n",
            "          Conv2d-162          [-1, 192, 14, 14]          37,056\n",
            "          Conv2d-163          [-1, 192, 14, 14]          37,056\n",
            "     BatchNorm2d-164          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-165          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-166          [-1, 192, 14, 14]             384\n",
            "            ReLU-167          [-1, 192, 14, 14]               0\n",
            "           block-168          [-1, 192, 14, 14]               0\n",
            "          Conv2d-169          [-1, 192, 14, 14]         331,968\n",
            "          Conv2d-170          [-1, 192, 14, 14]         331,968\n",
            "     BatchNorm2d-171          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-172          [-1, 192, 14, 14]             384\n",
            "          Conv2d-173          [-1, 192, 14, 14]          37,056\n",
            "          Conv2d-174          [-1, 192, 14, 14]          37,056\n",
            "     BatchNorm2d-175          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-176          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-177          [-1, 192, 14, 14]             384\n",
            "            ReLU-178          [-1, 192, 14, 14]               0\n",
            "           block-179          [-1, 192, 14, 14]               0\n",
            "          Conv2d-180          [-1, 192, 14, 14]         331,968\n",
            "          Conv2d-181          [-1, 192, 14, 14]         331,968\n",
            "     BatchNorm2d-182          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-183          [-1, 192, 14, 14]             384\n",
            "          Conv2d-184          [-1, 192, 14, 14]          37,056\n",
            "          Conv2d-185          [-1, 192, 14, 14]          37,056\n",
            "     BatchNorm2d-186          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-187          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-188          [-1, 192, 14, 14]             384\n",
            "            ReLU-189          [-1, 192, 14, 14]               0\n",
            "           block-190          [-1, 192, 14, 14]               0\n",
            "          Conv2d-191          [-1, 192, 14, 14]         331,968\n",
            "          Conv2d-192          [-1, 192, 14, 14]         331,968\n",
            "     BatchNorm2d-193          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-194          [-1, 192, 14, 14]             384\n",
            "          Conv2d-195          [-1, 192, 14, 14]          37,056\n",
            "          Conv2d-196          [-1, 192, 14, 14]          37,056\n",
            "     BatchNorm2d-197          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-198          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-199          [-1, 192, 14, 14]             384\n",
            "            ReLU-200          [-1, 192, 14, 14]               0\n",
            "           block-201          [-1, 192, 14, 14]               0\n",
            "          Conv2d-202          [-1, 192, 14, 14]         331,968\n",
            "          Conv2d-203          [-1, 192, 14, 14]         331,968\n",
            "     BatchNorm2d-204          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-205          [-1, 192, 14, 14]             384\n",
            "          Conv2d-206          [-1, 192, 14, 14]          37,056\n",
            "          Conv2d-207          [-1, 192, 14, 14]          37,056\n",
            "     BatchNorm2d-208          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-209          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-210          [-1, 192, 14, 14]             384\n",
            "            ReLU-211          [-1, 192, 14, 14]               0\n",
            "           block-212          [-1, 192, 14, 14]               0\n",
            "          Conv2d-213          [-1, 192, 14, 14]         331,968\n",
            "          Conv2d-214          [-1, 192, 14, 14]         331,968\n",
            "     BatchNorm2d-215          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-216          [-1, 192, 14, 14]             384\n",
            "          Conv2d-217          [-1, 192, 14, 14]          37,056\n",
            "          Conv2d-218          [-1, 192, 14, 14]          37,056\n",
            "     BatchNorm2d-219          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-220          [-1, 192, 14, 14]             384\n",
            "     BatchNorm2d-221          [-1, 192, 14, 14]             384\n",
            "            ReLU-222          [-1, 192, 14, 14]               0\n",
            "           block-223          [-1, 192, 14, 14]               0\n",
            "          Conv2d-224           [-1, 1280, 7, 7]       2,213,120\n",
            "          Conv2d-225           [-1, 1280, 7, 7]       2,213,120\n",
            "     BatchNorm2d-226           [-1, 1280, 7, 7]           2,560\n",
            "     BatchNorm2d-227           [-1, 1280, 7, 7]           2,560\n",
            "          Conv2d-228           [-1, 1280, 7, 7]         247,040\n",
            "          Conv2d-229           [-1, 1280, 7, 7]         247,040\n",
            "     BatchNorm2d-230           [-1, 1280, 7, 7]           2,560\n",
            "     BatchNorm2d-231           [-1, 1280, 7, 7]           2,560\n",
            "           block-232           [-1, 1280, 7, 7]               0\n",
            "AdaptiveAvgPool2d-233           [-1, 1280, 1, 1]               0\n",
            "          Linear-234                   [-1, 10]          12,810\n",
            "================================================================\n",
            "Total params: 15,681,066\n",
            "Trainable params: 15,681,066\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 136.39\n",
            "Params size (MB): 59.82\n",
            "Estimated Total Size (MB): 196.78\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhl0M7vUlTfc"
      },
      "source": [
        " model.reparametrize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dGq1l3bPtY7"
      },
      "source": [
        "depth = 2\n",
        "epochs = 1\n",
        "batch_size = 256\n",
        "base_lr = 0.01\n",
        "lr_decay = 0.0001\n",
        "milestones = '[80, 120]'\n",
        "device = \"cuda\"\n",
        "num_workers = 3\n",
        "\n",
        "#model = REPVGG(in_channels=3,num_classes=10,blocks=[2, 4, 14, 1],multipl=[0.75, 0.75, 0.75, 2.5]).to(device)\n",
        "#model = VGG(in_channels = 3, num_classes = 10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=base_lr)\n",
        "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=lr_decay)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=lr_decay)\n",
        "total_time = 0 \n",
        "for epoch in range(epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    start_time = time.time()\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images).to(device)\n",
        "    loss = criterion(outputs, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    if (i+1) % 250 == 0:\n",
        "      elapsed_time = time.time() - start_time\n",
        "      total_time += elapsed_time\n",
        "      print (\"Epoch {}, Step {} Loss: {:.4f} time : {:.4f}min\".format(epoch+1, i+1, loss.item(),total_time))\n",
        " # scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wxl6sTrtRZO",
        "outputId": "9a8dd6b8-dac9-45ed-88d1-1349185214e2"
      },
      "source": [
        "#after reparametrization \n",
        "from torchsummary import summary\n",
        "\n",
        "model_stats = summary(model.to(device), (3, 224, 224))\n",
        "summary_str = str(model_stats)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 48, 112, 112]           1,344\n",
            "             block-2         [-1, 48, 112, 112]               0\n",
            "            Conv2d-3           [-1, 48, 56, 56]          20,784\n",
            "             block-4           [-1, 48, 56, 56]               0\n",
            "            Conv2d-5           [-1, 48, 56, 56]          20,784\n",
            "             block-6           [-1, 48, 56, 56]               0\n",
            "            Conv2d-7           [-1, 96, 28, 28]          41,568\n",
            "             block-8           [-1, 96, 28, 28]               0\n",
            "            Conv2d-9           [-1, 96, 28, 28]          83,040\n",
            "            block-10           [-1, 96, 28, 28]               0\n",
            "           Conv2d-11           [-1, 96, 28, 28]          83,040\n",
            "            block-12           [-1, 96, 28, 28]               0\n",
            "           Conv2d-13           [-1, 96, 28, 28]          83,040\n",
            "            block-14           [-1, 96, 28, 28]               0\n",
            "           Conv2d-15          [-1, 192, 14, 14]         166,080\n",
            "            block-16          [-1, 192, 14, 14]               0\n",
            "           Conv2d-17          [-1, 192, 14, 14]         331,968\n",
            "            block-18          [-1, 192, 14, 14]               0\n",
            "           Conv2d-19          [-1, 192, 14, 14]         331,968\n",
            "            block-20          [-1, 192, 14, 14]               0\n",
            "           Conv2d-21          [-1, 192, 14, 14]         331,968\n",
            "            block-22          [-1, 192, 14, 14]               0\n",
            "           Conv2d-23          [-1, 192, 14, 14]         331,968\n",
            "            block-24          [-1, 192, 14, 14]               0\n",
            "           Conv2d-25          [-1, 192, 14, 14]         331,968\n",
            "            block-26          [-1, 192, 14, 14]               0\n",
            "           Conv2d-27          [-1, 192, 14, 14]         331,968\n",
            "            block-28          [-1, 192, 14, 14]               0\n",
            "           Conv2d-29          [-1, 192, 14, 14]         331,968\n",
            "            block-30          [-1, 192, 14, 14]               0\n",
            "           Conv2d-31          [-1, 192, 14, 14]         331,968\n",
            "            block-32          [-1, 192, 14, 14]               0\n",
            "           Conv2d-33          [-1, 192, 14, 14]         331,968\n",
            "            block-34          [-1, 192, 14, 14]               0\n",
            "           Conv2d-35          [-1, 192, 14, 14]         331,968\n",
            "            block-36          [-1, 192, 14, 14]               0\n",
            "           Conv2d-37          [-1, 192, 14, 14]         331,968\n",
            "            block-38          [-1, 192, 14, 14]               0\n",
            "           Conv2d-39          [-1, 192, 14, 14]         331,968\n",
            "            block-40          [-1, 192, 14, 14]               0\n",
            "           Conv2d-41          [-1, 192, 14, 14]         331,968\n",
            "            block-42          [-1, 192, 14, 14]               0\n",
            "           Conv2d-43           [-1, 1280, 7, 7]       2,213,120\n",
            "            block-44           [-1, 1280, 7, 7]               0\n",
            "AdaptiveAvgPool2d-45           [-1, 1280, 1, 1]               0\n",
            "           Linear-46                   [-1, 10]          12,810\n",
            "================================================================\n",
            "Total params: 7,041,194\n",
            "Trainable params: 7,041,194\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 27.38\n",
            "Params size (MB): 26.86\n",
            "Estimated Total Size (MB): 54.82\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkOkg0X-Rfqm"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct =0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy ( test images ) : {} %'.format(100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xOrNS1ux0nW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pQnWlvlPbOU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyHpeZTon8xE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}