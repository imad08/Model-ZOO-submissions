{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REPVGG.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJBqQgu3jRV4"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import os\n",
        "import time\n",
        "import importlib\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "import logging\n",
        "import argparse\n",
        "import numpy as np \n",
        "import random\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.backends.cudnn\n",
        "import torchvision.utils\n",
        "import torch.nn.functional as F\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXtbPAOLZjoW"
      },
      "source": [
        "**SIMPLE VGG ARCHITECTURE **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii--g1RqjS6P"
      },
      "source": [
        "architecture = [64 , 64 , -1 , 128 , 128 , -1 , 256 , 256, 256 , -1, 512, 512, 512 , -1 , 512 ,512, 512,-1]\n",
        "class VGG(nn.Module):\n",
        "   def __init__(self,in_channels=3,num_classes=10):\n",
        "    super(VGG,self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.mai = self.main_architecture(architecture)\n",
        "    self.fcs = nn.Sequential(nn.Linear(7*7*512,4096),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(p=0.5),\n",
        "                            nn.Linear(4096,4096),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(p=0.5),\n",
        "                            nn.Linear(4096,num_classes))                               \n",
        "   def forward(self,x):\n",
        "     x = self.mai(x)\n",
        "     x = x.reshape(x.shape[0], -1)\n",
        "     x = self.fcs(x)\n",
        "     return x \n",
        "   def main_architecture(self,architecture):\n",
        "            blocks = []\n",
        "            in_channels = self.in_channels\n",
        "            for layer in architecture:\n",
        "                      if layer != -1 : \n",
        "                        out_channels = layer\n",
        "                        blocks+=[nn.Conv2d(in_channels=in_channels,out_channels = out_channels , kernel_size = (3,3) , stride =(1,1) ,padding = (1,1) ),nn.BatchNorm2d(layer),nn.ReLU()]\n",
        "                        in_channels = layer \n",
        "                      elif layer == -1 :\n",
        "                        blocks+=[(nn.MaxPool2d(kernel_size=(2,2),stride= (2,2)))]\n",
        "            return nn.Sequential(*blocks)\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koy4hCaKdRbT"
      },
      "source": [
        "MAIN REPVGG Apart from main architecture specification I have also included identity and RELU as they were suggested for reparamatization in later half of paper also I have not included addition of Identity& ACB for Reparameterization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "dZzj1ULUIAka"
      },
      "source": [
        "\n",
        "\n",
        "class REPVGG(nn.Module):\n",
        "   def __init__(self,in_channels=3,num_classes=10,blocks=None,multipl=None):\n",
        "    super(REPVGG,self).__init__()\n",
        "    self.blocks= blocks\n",
        "    self.multipl= multipl \n",
        "    self.in_channels = in_channels\n",
        "    self.main_REPVGG = self.main_architecture()\n",
        "    self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "    self.linear = nn.Linear(int(512 * multipl[3]), num_classes)                               \n",
        "   \n",
        "   def main_architecture(self):\n",
        "            layers = []\n",
        "\n",
        "\n",
        "            #stage0\n",
        "\n",
        "            \n",
        "            multipl = self.multipl\n",
        "            out_channels = min(64,int(64*multipl[0])) \n",
        "            in_channels = self.in_channels\n",
        "            blocks = self.blocks\n",
        "            layers += [nn.Conv2d(in_channels = in_channels,out_channels=out_channels,kernel_size=(3,3),stride=(2,2),padding=(1,1)),nn.BatchNorm2d(num_features=out_channels),nn.Identity(),nn.ReLU()]\n",
        "            in_channels = min(64, int(64*multipl[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #stage1,stage2,stage3,stage4\n",
        "\n",
        "\n",
        "\n",
        "            for i in range(4):\n",
        "\n",
        "              out_channels = int(64*(2**i) * multipl[i])\n",
        "              layers += [nn.Conv2d(in_channels = in_channels,out_channels=out_channels,kernel_size=(3,3),stride=(2,2),padding=(1,1)),nn.BatchNorm2d(num_features=int(64*(2**i) * multipl[i])),nn.Identity(),nn.ReLU()]\n",
        "              in_channels = out_channels\n",
        "\n",
        "              for j in range(blocks[i]-1):\n",
        "                layers += [nn.Conv2d(in_channels = in_channels,out_channels=out_channels,kernel_size=(3,3),stride=(1,1),padding=(1,1)),nn.BatchNorm2d(num_features=out_channels),nn.Identity(),nn.ReLU()]\n",
        "              in_channels = out_channels \n",
        "\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "   def forward(self,x):\n",
        "     x = self.main_REPVGG(x)\n",
        "     x = x.view(x.size(0), -1)\n",
        "     x = self.linear(x)\n",
        "     return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To2a2qRGfCu6",
        "outputId": "161241be-5aca-4feb-bc05-268809ded697"
      },
      "source": [
        "transform = transforms.Compose([transforms.Pad(4),transforms.RandomHorizontalFlip(),transforms.RandomCrop(32),transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "test_transform  = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../../data/', train=True,transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../../data/', train=False,transform=test_transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=100, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpuncoWyZ5px"
      },
      "source": [
        "### **DIFFERENT TYPES OF REPVGG I have only implemented B2 since according to paper it giver best top1 accuracy all other can be implemented just by changing the arguments  **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N-oULfhZqcK"
      },
      "source": [
        "![download (3).png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYlT8FnDfCye",
        "outputId": "10cd4763-7de3-4893-fc1b-e62a88e82a88"
      },
      "source": [
        "depth = 2\n",
        "epochs = 8\n",
        "batch_size = 128\n",
        "base_lr = 0.01\n",
        "lr_decay = 0.1\n",
        "milestones = '[80, 120]'\n",
        "device = \"cuda\"\n",
        "num_workers = 3\n",
        "\n",
        "model = REPVGG(in_channels=3,num_classes=10,blocks=[4, 6, 16, 1],multipl=[2.5, 2.5, 2.5, 5]).to(device)\n",
        "#model = VGG(in_channels = 3, num_classes = 10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=base_lr)\n",
        "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=lr_decay)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=lr_decay)\n",
        "for epoch in range(epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (i+1) % 50 == 0:\n",
        "      print (\"Epoch {}, Step {} Loss: {:.4f}\".format(epoch+1, i+1, loss.item()))   \n",
        "  scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Step 50 Loss: 2.8460\n",
            "Epoch 1, Step 100 Loss: 2.2789\n",
            "Epoch 1, Step 150 Loss: 2.3100\n",
            "Epoch 1, Step 200 Loss: 4.9091\n",
            "Epoch 1, Step 250 Loss: 2.8399\n",
            "Epoch 1, Step 300 Loss: 2.7429\n",
            "Epoch 1, Step 350 Loss: 2.2851\n",
            "Epoch 1, Step 400 Loss: 2.3582\n",
            "Epoch 1, Step 450 Loss: 2.2819\n",
            "Epoch 1, Step 500 Loss: 2.2991\n",
            "Epoch 2, Step 50 Loss: 2.3177\n",
            "Epoch 2, Step 100 Loss: 2.3465\n",
            "Epoch 2, Step 150 Loss: 2.4544\n",
            "Epoch 2, Step 200 Loss: 2.2931\n",
            "Epoch 2, Step 250 Loss: 2.3105\n",
            "Epoch 2, Step 300 Loss: 2.3153\n",
            "Epoch 2, Step 350 Loss: 2.4150\n",
            "Epoch 2, Step 400 Loss: 2.4230\n",
            "Epoch 2, Step 450 Loss: 2.2830\n",
            "Epoch 2, Step 500 Loss: 2.2581\n",
            "Epoch 3, Step 50 Loss: 2.3315\n",
            "Epoch 3, Step 100 Loss: 2.4001\n",
            "Epoch 3, Step 150 Loss: 2.3537\n",
            "Epoch 3, Step 200 Loss: 2.2690\n",
            "Epoch 3, Step 250 Loss: 2.2731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq6efYEIIhcz",
        "outputId": "673cc6b5-04ff-492e-969f-563824cd880d"
      },
      "source": [
        "model.eval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.eval of REPVGG(\n",
              "  (main_REPVGG): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (4): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU()\n",
              "    (9): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (10): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): ReLU()\n",
              "    (12): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): ReLU()\n",
              "    (15): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (16): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (17): ReLU()\n",
              "    (18): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (20): ReLU()\n",
              "    (21): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (23): ReLU()\n",
              "    (24): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (26): ReLU()\n",
              "    (27): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (28): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (29): ReLU()\n",
              "    (30): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (32): ReLU()\n",
              "    (33): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (34): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (35): ReLU()\n",
              "    (36): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (37): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (38): ReLU()\n",
              "    (39): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (40): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (41): ReLU()\n",
              "    (42): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (43): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (44): ReLU()\n",
              "    (45): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (46): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (47): ReLU()\n",
              "    (48): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (49): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (50): ReLU()\n",
              "    (51): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (52): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (53): ReLU()\n",
              "    (54): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (55): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (56): ReLU()\n",
              "    (57): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (58): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (59): ReLU()\n",
              "    (60): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (61): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (62): ReLU()\n",
              "    (63): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (64): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (65): ReLU()\n",
              "    (66): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (67): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (68): ReLU()\n",
              "    (69): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (70): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (71): ReLU()\n",
              "    (72): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (73): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (74): ReLU()\n",
              "    (75): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (76): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (77): ReLU()\n",
              "    (78): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (79): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (80): ReLU()\n",
              "    (81): Conv2d(640, 2560, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (82): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (83): ReLU()\n",
              "  )\n",
              "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
              "  (linear): Linear(in_features=2560, out_features=10, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90iJdoKfjnWB"
      },
      "source": [
        "# IN 8 EPOCHS IT IS GIVING POOR ACCURACY I NEED TO HAVE BETTER DEVICE FOR 200 EPOCHS AS SUGGESTED IN PAPER "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix1t73kBiNLe",
        "outputId": "3723e22b-c97c-4a75-f584-7a912a98159f"
      },
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy ( test images ) : {} %'.format(100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy ( test images ) : 31.57 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQNHgnTnjJto"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}